{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchreid import models\n",
    "from collections import OrderedDict\n",
    "from torchvision.transforms import Normalize, ToTensor, Resize, Compose\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from tracker import Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO = 'video0015_cut.mp4'\n",
    "DETECTIONS = 'detections.json'\n",
    "OUTPUT = 'output.avi'\n",
    "DEVICE = torch.device('cuda:0')\n",
    "\n",
    "# make the video smaller by using:\n",
    "# ffmpeg -i output.avi -s 960x540 -c:v libx264 -c:a copy output.mp4 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'checkpoints/osnet_ain_x1_0_msmt17_256x128_amsgrad_ep50_lr0.0015_coslr_b64_fb10_softmax_labsmth_flip_jitter.pth'\n",
    "s = torch.load(p)\n",
    "s = OrderedDict([(k[7:], v) for k, v in s.items()])\n",
    "\n",
    "model = models.osnet_ain.osnet_ain_x1_0(num_classes=4101, pretrained=False)\n",
    "model.load_state_dict(s)\n",
    "\n",
    "model.fc[1] = torch.nn.Identity()\n",
    "model.fc[2] = torch.nn.Identity()\n",
    "model.classifier = torch.nn.Identity()\n",
    "model = model.eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "height = 256\n",
    "width = 128\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((height, width)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean, std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(frame, boxes):\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    descriptors = []\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xmin, ymin, xmax, ymax in boxes:\n",
    "            crop = frame_rgb[ymin:ymax, xmin:xmax]\n",
    "            x = transform(Image.fromarray(crop))\n",
    "            x = x.unsqueeze(0).to(DEVICE)\n",
    "            descriptors.append(model(x).cpu().numpy())\n",
    "\n",
    "    return np.concatenate(descriptors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box drawer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = ImageFont.truetype('/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', size=40)\n",
    "COLORS = {}\n",
    "TRACKS = {}\n",
    "\n",
    "def draw(image, boxes, ids):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: a numpy uint8 array with shape [h, w, 3].\n",
    "        boxes: a list of arrays with shape [4].\n",
    "    Returns:\n",
    "        a numpy uint8 array with shape [h, w, 3].\n",
    "    \"\"\"\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(image)\n",
    "    drawer = ImageDraw.Draw(image, 'RGBA')\n",
    "\n",
    "    for box, i in zip(boxes, ids):\n",
    "            \n",
    "        if i not in COLORS:\n",
    "            COLORS[i] = tuple(np.random.randint(0, 256, size=3))\n",
    "            \n",
    "        outline = COLORS[i]\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        box = [(xmin, ymin), (xmax, ymax)]\n",
    "        drawer.rectangle(box, outline=outline, width=10)\n",
    "        drawer.text(box[0], f'{i}', font=font)\n",
    "        \n",
    "        if i not in TRACKS:\n",
    "            TRACKS[i] = []\n",
    "            \n",
    "        TRACKS[i].append(((xmin + xmax)//2, ymin))\n",
    "        if len(TRACKS[i]) > 1:\n",
    "            for j, (x, y) in enumerate(TRACKS[i][:-1]):\n",
    "                x1, y1 = TRACKS[i][j+1]\n",
    "                drawer.line([(x, y), (x1, y1)], fill=outline + (150,), width=10)\n",
    "    \n",
    "    image = np.array(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DETECTIONS, 'r') as f:\n",
    "    detections = f.read()\n",
    "    detections = json.loads(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "times = []  # detection times\n",
    "tracker = Tracker(threshold=0.6, wait=4)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "num_frames = 10000#int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter(OUTPUT, fourcc, 1, (width, height))\n",
    "\n",
    "frame_id = 0\n",
    "for i in tqdm(range(num_frames)):\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    if i % (2 * fps) != 0:\n",
    "        continue\n",
    "\n",
    "    boxes = detections[str(i)]['boxes']\n",
    "    scores = detections[str(i)]['scores']\n",
    "    \n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "    boxes = boxes[scores > 0.9]\n",
    "    \n",
    "    if len(boxes) > 0:\n",
    "        ymin = boxes[:, 1]\n",
    "        boxes = boxes[(ymin/height) < 0.6]\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    descriptors = get_descriptors(frame, boxes)\n",
    "    tracks = tracker.update(boxes, descriptors)\n",
    "    boxes = [t['x'] for t in tracks if t['u'] == 1]\n",
    "    ids = [t['i'] for t in tracks if t['u'] == 1]\n",
    "    times.append(time.perf_counter() - start_time)\n",
    "\n",
    "    out_frame = draw(frame, boxes, ids)\n",
    "    out.write(out_frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "times = np.array(times[10:])\n",
    "print(times.mean(), times.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
